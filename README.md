# Fast Gradient Prototype for Differentiable Interference Superposition and Energy Integration

> This README describes the implementation and usage of the accompanying script `jit_1.py`, covering **phasor superposition → directional gain** and **total energy integration** objective functions with vectorized manual gradients, performance comparison between `autograd` and `torch.compile`.

---

## Background and Objectives

Many antenna/array modeling and beamforming problems can be abstracted into a two-layer structure:

1. **Phasor superposition (interference)** to obtain direction-dependent complex field $S$,
2. Construct scalar objective $y$ in the form of **total energy** or power for optimization.

This script implements complex arithmetic using **pure real tensors with separated real/imaginary parts**, constructing:

$
X(x) = c_0 + \sum_k c_{1,k}x_k + \sum_{i<j} c_{2,ij}x_ix_j + \cdots + \sum_{i_1<\cdots<i_5}c_{5,i_1\cdots i_5}\prod_t x_{i_t}
$

(supporting up to 5th-order terms, $x\in\mathbb{R}^K$, each $c_\bullet\in\mathbb{C}$), then applying linear transformation with complex matrix $b\in\mathbb{C}^{M\times N}$ to obtain

$$
S = b X \in \mathbb{C}^M, \qquad
y = \sum_{m=1}^{M} a_m |S_m|^2, \; a\in\mathbb{R}^M.
$$

The script provides **fully vectorized manual gradients** for **$\nabla_x y$** and compares correctness and performance with **PyTorch autograd** and (when available) **`torch.compile`**.

---

## Key Features

- **Complex numbers represented as real tensors**: $z=z_r+i z_i$ split into `*_r` and `*_i`, avoiding Python-level complex number overhead and backend compatibility issues.
- **Polynomial features $X(x)$**: Contains 0–5th order terms; high-order combinations generated by `itertools.combinations` with cached indices.
- **Two-layer chain rule differentiation**:
  - $\displaystyle \frac{\partial y}{\partial X_r}, \frac{\partial y}{\partial X_i}$ obtained analytically through energy objective formulation;
  - $\displaystyle \frac{\partial (X_r,X_i)}{\partial x}$ uses **vectorized** `scatter_add_` aggregation, eliminating Python for-loops.
- **Performance benchmarking**:
  - `manual` (fully vectorized manual gradients)
  - `autograd` (automatic differentiation baseline)
  - `compile` (`torch.compile` default mode, when available)
  - Additional **loop vs vectorized** comparison for `dX/dx` with maximum error checking.
- **Device adaptive**: Uses `cudaEvent` for precise timing and memory reporting on CUDA; falls back to CPU timing otherwise.

---

## Code Structure and Key Functions

- `compute_X_parts(...)`: Generate $X_r,X_i\in\mathbb{R}^N$ with real/imaginary separation.
- `compute_dX_parts_dx_vectorized(...)`: Core **vectorized** gradient module, returns $dX_r/dx, dX_i/dx \in \mathbb{R}^{N\times K}$.
- `compute_gradient_manual_real(x)`: Chains two-layer gradients, outputs $\nabla_x y\in\mathbb{R}^{N\times K}$.
- `forward_pass_real(x)`: Forward pass $y$ used only for `autograd` benchmark.
- `*_compiled(...)`: Fixed signature and `float32`-friendly versions for `torch.compile`.
- Benchmark utilities:
  - `test_dX_dx_performance(...)`: `loop` vs `vectorized` submodule comparison.
  - `time_gradient_computation(...)`: Multi-round timing and statistics for `manual / autograd / compile`.

> **Note**: `TorchScript` related code is commented out in the script; `torch.compile` is enabled only when PyTorch≥2.0 and runtime compilation succeeds.

---

## Mathematical Details (Implementation Notes)

### Objective Function Expansion (Real/Imaginary Separation)

$$
S_r = b_r X_r - b_i X_i, \quad S_i = b_r X_i + b_i X_r, \quad
y = \sum_m a_m (S_{r,m}^2 + S_{i,m}^2).
$$

This yields

$$
\frac{\partial y}{\partial X_r} = (2a\odot S_r)^\top b_r + (2a\odot S_i)^\top b_i, \quad
\frac{\partial y}{\partial X_i} = -(2a\odot S_r)^\top b_i + (2a\odot S_i)^\top b_r.
$$

### Monomial Derivatives with Respect to $x$

For index group $(i_1,\dots,i_p)$,

$$
\frac{\partial}{\partial x_{i_t}}\prod_{u}x_{i_u} = \frac{\prod_u x_{i_u}}{x_{i_t}} \quad(\text{when } x_{i_t} \neq 0).
$$

Implementation uses `full_prod/x_pos` and falls back to 0 for positions where $|x_{i_t}| < 1e-12$ (prioritizing numerical stability). For **strict analytical values**, this can be rewritten as "explicit omission of that factor" multiplication to avoid 0/0.

---

## Dimensions and Default Scale

- `M=100`, `N=1500`, `K=5` (modifiable at script top)
- `a: (M,)`, `b_r, b_i: (M,N)`, `x: (N,K)`
- `c1: (K,)`; `c2..c5` lengths are $C(K,2)$ to $C(K,5)$ respectively
- Output `grad: (N,K)`, i.e., local gradient for each row input `x[n,:]`

---

## Environment and Dependencies

- Python ≥ 3.8
- PyTorch ≥ **2.0** (recommended for `torch.compile`; lower versions can still run `manual/autograd`)
- GPU optional (auto-detected), CUDA must match PyTorch version

---

## Quick Start

```bash
# Recommended: create virtual environment and install PyTorch (choose command based on your platform/version)
python -m venv .venv && source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install --upgrade torch  # or follow https://pytorch.org

# Run the script
python jit_1.py
```

After running, you will see:

- Version and device information (whether `torch.compile` is supported, whether CUDA is used)
- `dX/dx`: **maximum difference** and **speedup ratio** between loop vs vectorized
- Complete gradients: **mean±std**, **fastest/slowest**, **norm** and **relative error** for `manual / autograd / compile`
- (If on GPU) Memory usage statistics and parameter count estimation for problem scale

> **Tip**: The script defaults to 15 test rounds, discarding the first timing sample from each group to reduce warm-up effects. You can adjust `num_runs` at the end.

---

## Mapping Script to EM/Array Scenarios

- Let $x$ represent differentiable design variables such as direction/frequency/element phases (or their feature mappings),
- $X(x)$ serves as **differentiable "feature polynomials"** derived from $x$ (can be replaced with more physical phase terms or direction cosine combinations),
- $b$ can correspond to **array manifold/propagation matrices** or numerical discretization operators,
- $y$ constructed using **energy/power criteria** (supports adding weights $a$), turning beamforming/robust design into gradient-driven optimization problems.

---

## Performance and Numerical Considerations

- **Numerical stability**: When $x$ components approach 0, derivative terms in `full_prod/x_pos` are set to 0; if you prefer **analytical limits**, this can be implemented as "explicit multiplication omitting that variable".
- **Data types**: Primarily uses `float32`; can switch to `float64` as needed (note memory and performance implications).
- **`torch.compile`**: First run includes compilation overhead; script includes simple warm-up and only includes in benchmarks when available/successful.

---

## How to Extend

- Support higher-order terms or switch to **sparse selection** (more friendly for high-dimensional $K$)
- Direct implementation using `torch.complex64` for complex number path (easier interface with other libraries)
- Introduce **physical priors**: beam pattern templates, array manifolds $a(\theta)$, bandwidth/phase constraints, etc.
- Replace `y` with more engineering-relevant objectives (sidelobe suppression, 2-norm/∞-norm constraints, robustness terms, etc.)

---

## License and Acknowledgments

- **License**: Can add `MIT`/`BSD-3` or other open-source licenses as needed.
- **Acknowledgments**: Thanks to the PyTorch community for continuous optimization of `torch.compile` and tensor operators.

---

## Frequently Asked Questions (FAQ)

**Q: Is GPU required?**
> A: No. CPU can also run, but performance will be inferior to GPU for large-scale problems.

**Q: `torch.compile` errors or no acceleration?**
> A: Different platforms/drivers/graphics backends have varying benefits from `compile`. The script will automatically fall back without affecting correctness. Try updating PyTorch or changing `mode` (example code already reserved).

**Q: Why not use complex dtype?**
> A: This prototype prioritizes "portability + compilability" and demonstrates "real/imaginary separation + vectorization" general techniques; if your stack has complete complex number support, you can certainly switch implementations.
