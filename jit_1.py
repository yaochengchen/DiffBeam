import torch
from itertools import combinations
import time
import sys

# -------------------- ç¯å¢ƒä¸æ•°æ® --------------------
TORCH_VERSION = tuple(int(x) for x in torch.__version__.split('.')[:2])
HAS_COMPILE = TORCH_VERSION >= (2, 0)
print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"æ”¯æŒ torch.compile: {HAS_COMPILE}")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# è§„æ¨¡
M = 100
N = 1500
K = 5

torch.manual_seed(42)

# a_i: å®æ•° (M,)
a = torch.randn(M, dtype=torch.float32, device=device)

# b: å¤æ•°çŸ©é˜µçš„å®/è™šæ‹†åˆ†ï¼ˆåªä¿ç•™å®æ•°å¼ é‡ï¼‰
b_r = torch.randn(M, N, dtype=torch.float32, device=device)
b_i = torch.randn(M, N, dtype=torch.float32, device=device) + 0.1

# x_{j,k}: å®æ•°è¾“å…¥ (N,K)
x = torch.randn(N, K, dtype=torch.float32, device=device)

# å¤ç³»æ•°çš„å®/è™šæ‹†åˆ†ï¼ˆä»…ç”¨å®æ•°å­˜å‚¨ï¼‰
def make_c(n):
    r = torch.randn(n, device=device, dtype=torch.float32)
    i = torch.randn(n, device=device, dtype=torch.float32)
    return r, i

c0_r, c0_i = make_c(1)     # æ ‡é‡å­˜æˆ1å…ƒç´ å‘é‡ï¼Œåç»­ç”¨ .item()
c1_r, c1_i = make_c(K)

idx2 = list(combinations(range(K), 2))
idx3 = list(combinations(range(K), 3))
idx4 = list(combinations(range(K), 4))
idx5 = list(combinations(range(K), 5))

c2_r, c2_i = make_c(len(idx2))
c3_r, c3_i = make_c(len(idx3))
c4_r, c4_i = make_c(len(idx4))
c5_r, c5_i = make_c(len(idx5))

# é¢„å…ˆç¼“å­˜ç´¢å¼•å¼ é‡ï¼ˆé¿å…çƒ­è·¯å¾„åˆ›å»ºï¼‰
idx2_tensor = torch.tensor(idx2, device=device, dtype=torch.long) if idx2 else torch.empty(0, 2, dtype=torch.long, device=device)
idx3_tensor = torch.tensor(idx3, device=device, dtype=torch.long) if idx3 else torch.empty(0, 3, dtype=torch.long, device=device)
idx4_tensor = torch.tensor(idx4, device=device, dtype=torch.long) if idx4 else torch.empty(0, 4, dtype=torch.long, device=device)
idx5_tensor = torch.tensor(idx5, device=device, dtype=torch.long) if idx5 else torch.empty(0, 5, dtype=torch.long, device=device)

# -------------------- å®/è™šåˆ†ç¦»çš„å‰å‘ä¸æ¢¯åº¦ --------------------
def compute_X_parts(x_in,
                    c0_r, c0_i,
                    c1_r, c1_i,
                    c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
                    idx2_t, idx3_t, idx4_t, idx5_t):
    """è¿”å› (X_r, X_i)ï¼Œå‡ä¸º (N,) float32"""
    N = x_in.shape[0]
    X_r = torch.full((N,), c0_r.item(), dtype=torch.float32, device=x_in.device)
    X_i = torch.full((N,), c0_i.item(), dtype=torch.float32, device=x_in.device)

    # ä¸€æ¬¡é¡¹
    X_r += x_in @ c1_r
    X_i += x_in @ c1_i

    # é«˜æ¬¡é¡¹ï¼šå…ˆç®—å®æ•°å•é¡¹å¼ prodï¼Œå†åˆ†åˆ«ä¹˜ä»¥å®/è™šç³»æ•°
    if idx2_t.numel() > 0:
        prod2 = torch.prod(x_in[:, idx2_t], dim=2)  # (N,C2)
        X_r += prod2 @ c2_r
        X_i += prod2 @ c2_i
    if idx3_t.numel() > 0:
        prod3 = torch.prod(x_in[:, idx3_t], dim=2)  # (N,C3)
        X_r += prod3 @ c3_r
        X_i += prod3 @ c3_i
    if idx4_t.numel() > 0:
        prod4 = torch.prod(x_in[:, idx4_t], dim=2)  # (N,C4)
        X_r += prod4 @ c4_r
        X_i += prod4 @ c4_i
    if idx5_t.numel() > 0:
        prod5 = torch.prod(x_in[:, idx5_t], dim=2)  # (N,C5)
        X_r += prod5 @ c5_r
        X_i += prod5 @ c5_i

    return X_r, X_i

def compute_dX_parts_dx_original(x_in,
                                 c1_r, c1_i,
                                 c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
                                 idx2, idx3, idx4, idx5):
    """loop ç‰ˆæœ¬ dX/dxï¼ˆç”¨äºå¯¹æ¯”ï¼‰ï¼Œè¿”å› (dXr_dx, dXi_dx)ï¼Œå½¢çŠ¶ (N,K)"""
    N, K = x_in.shape
    dXr_dx = torch.zeros(N, K, dtype=torch.float32, device=x_in.device)
    dXi_dx = torch.zeros_like(dXr_dx)

    # ä¸€æ¬¡é¡¹
    for k in range(K):
        dXr_dx[:, k] += c1_r[k]
        dXi_dx[:, k] += c1_i[k]

    # äºŒæ¬¡é¡¹
    for i, (k1, k2) in enumerate(idx2):
        dXr_dx[:, k1] += c2_r[i] * x_in[:, k2]
        dXr_dx[:, k2] += c2_r[i] * x_in[:, k1]
        dXi_dx[:, k1] += c2_i[i] * x_in[:, k2]
        dXi_dx[:, k2] += c2_i[i] * x_in[:, k1]

    # ä¸‰æ¬¡é¡¹
    for i, (k1, k2, k3) in enumerate(idx3):
        r, ii = c3_r[i], c3_i[i]
        x1, x2, x3 = x_in[:, k1], x_in[:, k2], x_in[:, k3]
        dXr_dx[:, k1] += r * x2 * x3
        dXr_dx[:, k2] += r * x1 * x3
        dXr_dx[:, k3] += r * x1 * x2
        dXi_dx[:, k1] += ii * x2 * x3
        dXi_dx[:, k2] += ii * x1 * x3
        dXi_dx[:, k3] += ii * x1 * x2

    # å››æ¬¡é¡¹
    for i, (k1, k2, k3, k4) in enumerate(idx4):
        r, ii = c4_r[i], c4_i[i]
        x1, x2, x3, x4 = x_in[:, k1], x_in[:, k2], x_in[:, k3], x_in[:, k4]
        dXr_dx[:, k1] += r * (x2 * x3 * x4)
        dXr_dx[:, k2] += r * (x1 * x3 * x4)
        dXr_dx[:, k3] += r * (x1 * x2 * x4)
        dXr_dx[:, k4] += r * (x1 * x2 * x3)
        dXi_dx[:, k1] += ii * (x2 * x3 * x4)
        dXi_dx[:, k2] += ii * (x1 * x3 * x4)
        dXi_dx[:, k3] += ii * (x1 * x2 * x4)
        dXi_dx[:, k4] += ii * (x1 * x2 * x3)

    # äº”æ¬¡é¡¹
    for i, (k1, k2, k3, k4, k5) in enumerate(idx5):
        r, ii = c5_r[i], c5_i[i]
        x1, x2, x3, x4, x5 = x_in[:, k1], x_in[:, k2], x_in[:, k3], x_in[:, k4], x_in[:, k5]
        dXr_dx[:, k1] += r * (x2 * x3 * x4 * x5)
        dXr_dx[:, k2] += r * (x1 * x3 * x4 * x5)
        dXr_dx[:, k3] += r * (x1 * x2 * x4 * x5)
        dXr_dx[:, k4] += r * (x1 * x2 * x3 * x5)
        dXr_dx[:, k5] += r * (x1 * x2 * x3 * x4)
        dXi_dx[:, k1] += ii * (x2 * x3 * x4 * x5)
        dXi_dx[:, k2] += ii * (x1 * x3 * x4 * x5)
        dXi_dx[:, k3] += ii * (x1 * x2 * x4 * x5)
        dXi_dx[:, k4] += ii * (x1 * x2 * x3 * x5)
        dXi_dx[:, k5] += ii * (x1 * x2 * x3 * x4)

    return dXr_dx, dXi_dx

def compute_dX_parts_dx_vectorized(x_in,
                                   c1_r, c1_i,
                                   c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
                                   idx2_t, idx3_t, idx4_t, idx5_t):
    """å‘é‡åŒ– dX/dxï¼Œè¿”å› (dXr_dx, dXi_dx)"""
    N, K = x_in.shape
    dXr_dx = torch.zeros(N, K, dtype=torch.float32, device=x_in.device)
    dXi_dx = torch.zeros_like(dXr_dx)

    # ä¸€æ¬¡é¡¹
    dXr_dx += c1_r.unsqueeze(0)  # (N,K)
    dXi_dx += c1_i.unsqueeze(0)

    def add_order(indices_t, coeff_r, coeff_i, order):
        nonlocal dXr_dx, dXi_dx
        if indices_t.numel() == 0:
            return
        x_vals = x_in[:, indices_t]                # (N, C, order)
        full_prod = torch.prod(x_vals, dim=2)      # (N, C)
        for pos in range(order):
            x_pos = x_vals[:, :, pos]              # (N, C)
            # é¿å…é™¤é›¶ï¼šé›¶ä½ç½®çš„å¯¼æ•°ç›´æ¥æŒ‰â€œå»æ‰è¯¥å˜é‡åçš„ä¹˜ç§¯â€=0å¤„ç†
            partial = torch.where(torch.abs(x_pos) > 1e-12,
                                  full_prod / x_pos,
                                  torch.zeros_like(full_prod))
            # æƒé‡
            wr = coeff_r.unsqueeze(0) * partial    # (N, C)
            wi = coeff_i.unsqueeze(0) * partial
            var_idx = indices_t[:, pos].unsqueeze(0).expand(N, -1)  # (N, C)
            dXr_dx.scatter_add_(1, var_idx, wr)
            dXi_dx.scatter_add_(1, var_idx, wi)

    add_order(idx2_t, c2_r, c2_i, 2)
    add_order(idx3_t, c3_r, c3_i, 3)
    add_order(idx4_t, c4_r, c4_i, 4)
    add_order(idx5_t, c5_r, c5_i, 5)

    return dXr_dx, dXi_dx

def forward_pass_real(x_input):
    """çº¯å®æ•°å‰å‘ï¼ˆç”¨äº autograd åŸºå‡†ï¼‰"""
    X_r, X_i = compute_X_parts(
        x_input, c0_r, c0_i, c1_r, c1_i,
        c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
        idx2_tensor, idx3_tensor, idx4_tensor, idx5_tensor
    )
    S_r = b_r @ X_r - b_i @ X_i
    S_i = b_r @ X_i + b_i @ X_r
    y = torch.sum(a * (S_r * S_r + S_i * S_i))
    return y

def compute_gradient_manual_real(x_in):
    """æ‰‹åŠ¨æ¢¯åº¦ï¼ˆçº¯å®æ•°é“¾å¼ï¼‰"""
    X_r, X_i = compute_X_parts(
        x_in, c0_r, c0_i, c1_r, c1_i,
        c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
        idx2_tensor, idx3_tensor, idx4_tensor, idx5_tensor
    )
    S_r = b_r @ X_r - b_i @ X_i
    S_i = b_r @ X_i + b_i @ X_r

    w_r = 2.0 * a * S_r   # (M,)
    w_i = 2.0 * a * S_i   # (M,)

    dy_dX_r = w_r @ b_r + w_i @ b_i     # (N,)
    dy_dX_i = -w_r @ b_i + w_i @ b_r    # (N,)

    dXr_dx, dXi_dx = compute_dX_parts_dx_vectorized(
        x_in, c1_r, c1_i, c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
        idx2_tensor, idx3_tensor, idx4_tensor, idx5_tensor
    )
    grad = dy_dX_r.unsqueeze(1) * dXr_dx + dy_dX_i.unsqueeze(1) * dXi_dx
    return grad

# -------------------- ç¼–è¯‘å‹å¥½ç‰ˆæœ¬ï¼ˆç­¾åå›ºå®š + ä»… float32ï¼‰ --------------------
def compute_X_parts_compiled(x_in: torch.Tensor,
                             c0_r: torch.Tensor, c0_i: torch.Tensor,
                             c1_r: torch.Tensor, c1_i: torch.Tensor,
                             c2_r: torch.Tensor, c2_i: torch.Tensor,
                             c3_r: torch.Tensor, c3_i: torch.Tensor,
                             c4_r: torch.Tensor, c4_i: torch.Tensor,
                             c5_r: torch.Tensor, c5_i: torch.Tensor,
                             idx2_t: torch.Tensor, idx3_t: torch.Tensor,
                             idx4_t: torch.Tensor, idx5_t: torch.Tensor):
    N = x_in.shape[0]
    X_r = torch.full((N,), float(c0_r.item()), dtype=torch.float32, device=x_in.device)
    X_i = torch.full((N,), float(c0_i.item()), dtype=torch.float32, device=x_in.device)

    X_r = X_r + (x_in @ c1_r)
    X_i = X_i + (x_in @ c1_i)

    if idx2_t.numel() > 0:
        prod2 = torch.prod(x_in[:, idx2_t], dim=2)
        X_r = X_r + prod2 @ c2_r
        X_i = X_i + prod2 @ c2_i
    if idx3_t.numel() > 0:
        prod3 = torch.prod(x_in[:, idx3_t], dim=2)
        X_r = X_r + prod3 @ c3_r
        X_i = X_i + prod3 @ c3_i
    if idx4_t.numel() > 0:
        prod4 = torch.prod(x_in[:, idx4_t], dim=2)
        X_r = X_r + prod4 @ c4_r
        X_i = X_i + prod4 @ c4_i
    if idx5_t.numel() > 0:
        prod5 = torch.prod(x_in[:, idx5_t], dim=2)
        X_r = X_r + prod5 @ c5_r
        X_i = X_i + prod5 @ c5_i

    return X_r, X_i

def compute_dX_parts_dx_compiled(x_in: torch.Tensor,
                                 c1_r: torch.Tensor, c1_i: torch.Tensor,
                                 c2_r: torch.Tensor, c2_i: torch.Tensor,
                                 c3_r: torch.Tensor, c3_i: torch.Tensor,
                                 c4_r: torch.Tensor, c4_i: torch.Tensor,
                                 c5_r: torch.Tensor, c5_i: torch.Tensor,
                                 idx2_t: torch.Tensor, idx3_t: torch.Tensor,
                                 idx4_t: torch.Tensor, idx5_t: torch.Tensor):
    N, K = x_in.shape
    dXr_dx = torch.zeros(N, K, dtype=torch.float32, device=x_in.device)
    dXi_dx = torch.zeros_like(dXr_dx)

    dXr_dx = dXr_dx + c1_r.unsqueeze(0)
    dXi_dx = dXi_dx + c1_i.unsqueeze(0)

    def add_order(indices_t, coeff_r, coeff_i, order):
        nonlocal dXr_dx, dXi_dx
        if indices_t.numel() == 0:
            return
        x_vals = x_in[:, indices_t]           # (N,C,order)
        full_prod = torch.prod(x_vals, dim=2) # (N,C)
        for pos in range(order):
            x_pos = x_vals[:, :, pos]
            partial = torch.where(torch.abs(x_pos) > 1e-12,
                                  full_prod / x_pos,
                                  torch.zeros_like(full_prod))
            wr = coeff_r.unsqueeze(0) * partial
            wi = coeff_i.unsqueeze(0) * partial
            var_idx = indices_t[:, pos].unsqueeze(0).expand(N, -1)
            dXr_dx.scatter_add_(1, var_idx, wr)
            dXi_dx.scatter_add_(1, var_idx, wi)

    add_order(idx2_t, c2_r, c2_i, 2)
    add_order(idx3_t, c3_r, c3_i, 3)
    add_order(idx4_t, c4_r, c4_i, 4)
    add_order(idx5_t, c5_r, c5_i, 5)

    return dXr_dx, dXi_dx

def compute_gradient_manual_compiled_real(x_in: torch.Tensor, a: torch.Tensor,
                                          b_r: torch.Tensor, b_i: torch.Tensor,
                                          c0_r: torch.Tensor, c0_i: torch.Tensor,
                                          c1_r: torch.Tensor, c1_i: torch.Tensor,
                                          c2_r: torch.Tensor, c2_i: torch.Tensor,
                                          c3_r: torch.Tensor, c3_i: torch.Tensor,
                                          c4_r: torch.Tensor, c4_i: torch.Tensor,
                                          c5_r: torch.Tensor, c5_i: torch.Tensor,
                                          idx2_t: torch.Tensor, idx3_t: torch.Tensor,
                                          idx4_t: torch.Tensor, idx5_t: torch.Tensor):
    X_r, X_i = compute_X_parts_compiled(
        x_in, c0_r, c0_i, c1_r, c1_i, c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
        idx2_t, idx3_t, idx4_t, idx5_t
    )

    S_r = b_r @ X_r - b_i @ X_i
    S_i = b_r @ X_i + b_i @ X_r

    w_r = 2.0 * a * S_r
    w_i = 2.0 * a * S_i

    dy_dX_r = w_r @ b_r + w_i @ b_i
    dy_dX_i = -w_r @ b_i + w_i @ b_r

    dXr_dx, dXi_dx = compute_dX_parts_dx_compiled(
        x_in, c1_r, c1_i, c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
        idx2_t, idx3_t, idx4_t, idx5_t
    )

    grad = dy_dX_r.unsqueeze(1) * dXr_dx + dy_dX_i.unsqueeze(1) * dXi_dx
    return grad

# -------------------- åˆ›å»ºç¼–è¯‘ç‰ˆæœ¬ --------------------
HAS_JIT = False
'''
try:
    compute_gradient_jit = torch.jit.script(compute_gradient_manual_compiled_real)
    print("âœ“ TorchScript ç¼–è¯‘æˆåŠŸ")
    HAS_JIT = True
except Exception as e:
    print(f"âœ— TorchScript ç¼–è¯‘å¤±è´¥: {e}")
    HAS_JIT = False
'''
if HAS_COMPILE:
    try:
        compute_gradient_compile_default = torch.compile(compute_gradient_manual_compiled_real)
        compute_gradient_compile_reduce_overhead = torch.compile(compute_gradient_manual_compiled_real, mode="reduce-overhead")
        compute_gradient_compile_max_autotune = torch.compile(compute_gradient_manual_compiled_real, mode="max-autotune")
        print("âœ“ torch.compile ç¼–è¯‘æˆåŠŸ")
        HAS_TORCH_COMPILE = True
    except Exception as e:
        print(f"âœ— torch.compile ç¼–è¯‘å¤±è´¥: {e}")
        HAS_TORCH_COMPILE = False
else:
    HAS_TORCH_COMPILE = False

# -------------------- æ€§èƒ½æµ‹è¯•å·¥å…· --------------------
def time_gradient_computation(method='manual', num_runs=10):
    times = []
    # é¢„çƒ­
    if method in ['jit', 'compile', 'compile_reduce', 'compile_max']:
        try:
            if method == 'jit' and HAS_JIT:
                _ = compute_gradient_jit(x, a, b_r, b_i,
                                         c0_r, c0_i, c1_r, c1_i,
                                         c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
                                         idx2_tensor, idx3_tensor, idx4_tensor, idx5_tensor)
            elif method == 'compile' and HAS_TORCH_COMPILE:
                _ = compute_gradient_compile_default(x, a, b_r, b_i,
                                                     c0_r, c0_i, c1_r, c1_i,
                                                     c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
                                                     idx2_tensor, idx3_tensor, idx4_tensor, idx5_tensor)
            elif method == 'compile_reduce' and HAS_TORCH_COMPILE:
                _ = compute_gradient_compile_reduce_overhead(x, a, b_r, b_i,
                                                             c0_r, c0_i, c1_r, c1_i,
                                                             c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
                                                             idx2_tensor, idx3_tensor, idx4_tensor, idx5_tensor)
            elif method == 'compile_max' and HAS_TORCH_COMPILE:
                _ = compute_gradient_compile_max_autotune(x, a, b_r, b_i,
                                                          c0_r, c0_i, c1_r, c1_i,
                                                          c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
                                                          idx2_tensor, idx3_tensor, idx4_tensor, idx5_tensor)
        except Exception as e:
            print(f"é¢„çƒ­å¤±è´¥: {e}")
            return [], torch.zeros_like(x)

    for _ in range(num_runs):
        if device.type == 'cuda':
            torch.cuda.synchronize()
            start_event = torch.cuda.Event(enable_timing=True)
            end_event = torch.cuda.Event(enable_timing=True)
            start_event.record()
        else:
            start_time = time.time()

        try:
            if method == 'manual':
                gradient = compute_gradient_manual_real(x)
            elif method == 'jit' and HAS_JIT:
                gradient = compute_gradient_jit(x, a, b_r, b_i,
                                                c0_r, c0_i, c1_r, c1_i,
                                                c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
                                                idx2_tensor, idx3_tensor, idx4_tensor, idx5_tensor)
            elif method == 'compile' and HAS_TORCH_COMPILE:
                gradient = compute_gradient_compile_default(x, a, b_r, b_i,
                                                            c0_r, c0_i, c1_r, c1_i,
                                                            c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
                                                            idx2_tensor, idx3_tensor, idx4_tensor, idx5_tensor)
            elif method == 'compile_reduce' and HAS_TORCH_COMPILE:
                gradient = compute_gradient_compile_reduce_overhead(x, a, b_r, b_i,
                                                                    c0_r, c0_i, c1_r, c1_i,
                                                                    c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
                                                                    idx2_tensor, idx3_tensor, idx4_tensor, idx5_tensor)
            elif method == 'compile_max' and HAS_TORCH_COMPILE:
                gradient = compute_gradient_compile_max_autotune(x, a, b_r, b_i,
                                                                 c0_r, c0_i, c1_r, c1_i,
                                                                 c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
                                                                 idx2_tensor, idx3_tensor, idx4_tensor, idx5_tensor)
            else:  # autograd åŸºå‡†
                x_temp = x.clone().requires_grad_(True)
                y = forward_pass_real(x_temp)
                y.backward()
                gradient = x_temp.grad
        except Exception as e:
            print(f"è®¡ç®—å¤±è´¥({method}): {e}")
            return [], torch.zeros_like(x)

        if device.type == 'cuda':
            end_event.record()
            torch.cuda.synchronize()
            elapsed = start_event.elapsed_time(end_event)
        else:
            elapsed = (time.time() - start_time) * 1000.0

        times.append(elapsed)

    return times, gradient

# -------------------- å­æ¨¡å—æ€§èƒ½å¯¹æ¯”ï¼ˆdX/dxï¼‰ --------------------
def test_dX_dx_performance(use_vectorized=True, num_runs=10):
    times = []
    for _ in range(num_runs):
        if device.type == 'cuda':
            torch.cuda.synchronize()
            start_event = torch.cuda.Event(enable_timing=True)
            end_event = torch.cuda.Event(enable_timing=True)
            start_event.record()
        else:
            start_time = time.time()

        if use_vectorized:
            result_r, result_i = compute_dX_parts_dx_vectorized(
                x, c1_r, c1_i, c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
                idx2_tensor, idx3_tensor, idx4_tensor, idx5_tensor
            )
        else:
            result_r, result_i = compute_dX_parts_dx_original(
                x, c1_r, c1_i, c2_r, c2_i, c3_r, c3_i, c4_r, c4_i, c5_r, c5_i,
                idx2, idx3, idx4, idx5
            )

        if device.type == 'cuda':
            end_event.record()
            torch.cuda.synchronize()
            elapsed = start_event.elapsed_time(end_event)
        else:
            elapsed = (time.time() - start_time) * 1000.0
        times.append(elapsed)

    return times, (result_r, result_i)

# -------------------- è¿è¡Œæ€§èƒ½æµ‹è¯• --------------------
num_runs = 15
print(f"\nè¿›è¡Œ {num_runs} æ¬¡æµ‹è¯•ä»¥è·å¾—ç¨³å®šçš„æ—¶é—´æµ‹é‡...")

print("\n" + "="*60)
print("dX/dxï¼šloop vs å‘é‡åŒ–ï¼ˆçº¯å®æ•°ï¼‰")
print("="*60)
orig_times, (orig_r, orig_i) = test_dX_dx_performance(False, num_runs)
vec_times, (vec_r, vec_i) = test_dX_dx_performance(True, num_runs)

orig_avg = sum(orig_times) / len(orig_times)
vec_avg  = sum(vec_times)  / len(vec_times)

max_diff_dx = max(
    torch.max(torch.abs(orig_r - vec_r)).item(),
    torch.max(torch.abs(orig_i - vec_i)).item()
)
print(f"æœ€å¤§å·®å¼‚: {max_diff_dx:.2e}")
print("âœ“ å‘é‡åŒ– dX/dx æ­£ç¡®" if max_diff_dx < 1e-6 else "âš ï¸ å­˜åœ¨åå·®")
print(f"åŸå§‹å¹³å‡: {orig_avg:.2f} ms | å‘é‡åŒ–å¹³å‡: {vec_avg:.2f} ms | åŠ é€Ÿæ¯”: {orig_avg/vec_avg:.2f}x")

print("\n" + "="*60)
print("å®Œæ•´æ¢¯åº¦ï¼šmanual / autograd / JIT / compileï¼ˆçº¯å®æ•°ï¼‰")
print("="*60)

test_methods = ['manual', 'autograd', 'compile']
#if HAS_JIT: test_methods.append('jit')
#if HAS_TORCH_COMPILE: test_methods += ['compile', 'compile_reduce', 'compile_max']

results = {}
names = {
    'manual': 'å®Œå…¨å‘é‡åŒ–æ‰‹åŠ¨æ¢¯åº¦',
    'autograd': 'è‡ªåŠ¨å¾®åˆ†æ¢¯åº¦',
    'jit': 'TorchScript JIT',
    'compile': 'torch.compile (default)',
    'compile_reduce': 'torch.compile (reduce-overhead)',
    'compile_max': 'torch.compile (max-autotune)'
}
print("æµ‹è¯•æ–¹æ³•:", ', '.join(names[m] for m in test_methods), "\n")

for method in test_methods:
    print(f"æµ‹è¯• {names[method]}...")
    times, grad = time_gradient_computation(method, num_runs)
    times = times[1:]
    if times:
        avg = sum(times) / len(times)
        std = (sum((t-avg)**2 for t in times)/len(times))**0.5
        results[method] = {
            'times': times, 'avg_time': avg, 'std_time': std,
            'gradient': grad, 'min_time': min(times), 'max_time': max(times)
        }
        print(f"  å¹³å‡: {avg:.2f} Â± {std:.2f} ms | èŒƒå›´: {min(times):.2f}-{max(times):.2f} ms | èŒƒæ•°: {torch.norm(grad).item():.6f}\n")
    else:
        print("  æµ‹è¯•å¤±è´¥\n")

print("="*60)
print("æ€§èƒ½å¯¹æ¯”ä¸æ­£ç¡®æ€§ï¼ˆä»¥ autograd ä¸ºåŸºå‡†ï¼‰")
print("="*60)
if 'autograd' in results:
    base_grad = results['autograd']['gradient']
    base_time = results['autograd']['avg_time']
    for m, d in results.items():
        if m == 'autograd': continue
        mdiff = torch.max(torch.abs(d['gradient'] - base_grad)).item()
        rerr  = (torch.norm(d['gradient'] - base_grad) / torch.norm(base_grad)).item()
        speed = base_time / d['avg_time']
        save  = base_time - d['avg_time']
        pct   = save / base_time * 100
        print(f"\n{names[m]}:")
        print(f"  æ­£ç¡®æ€§: max {mdiff:.2e}, rel {rerr:.2e} {'âœ“' if mdiff<1e-5 else 'âš ï¸'}")
        print(f"  æ€§èƒ½: {speed:.2f}xï¼Œ{'èŠ‚çœ' if speed>1 else 'å¢åŠ '}æ—¶é—´ {abs(save):.2f} ms ({abs(pct):.1f}%)")

print("\nè¯¦ç»†ç»Ÿè®¡")
print("-"*40)
for m, d in results.items():
    print(f"{names[m]}: æœ€å¿« {d['min_time']:.2f} ms | æœ€æ…¢ {d['max_time']:.2f} ms | å¹³å‡ {d['avg_time']:.2f} ms | Ïƒ={d['std_time']:.2f} ms")

if results:
    fastest = min(results, key=lambda k: results[k]['avg_time'])
    print(f"\nğŸ† æœ€å¿«æ–¹æ³•: {names[fastest]} ({results[fastest]['avg_time']:.2f} ms)")

if device.type == 'cuda':
    mem_used = torch.cuda.memory_allocated(device) / 1024**2
    mem_rsvd = torch.cuda.memory_reserved(device) / 1024**2
    print(f"\nGPUå†…å­˜ä½¿ç”¨: {mem_used:.2f} MB (ç¼“å­˜: {mem_rsvd:.2f} MB)")
else:
    print("\nè¿è¡Œæ¨¡å¼: CPU")

print(f"\né—®é¢˜è§„æ¨¡: M={M}, N={N}, K={K}")
print(f"æ€»å‚æ•°é‡(ä¼°ç®—): M + 2*M*N + 2*sum(C(K,i), i=2..5) + 2*K + 2 = "
      f"{M + 2*M*N + 2*(len(idx2)+len(idx3)+len(idx4)+len(idx5)) + 2*K + 2}")
